{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier for Spam Detection\n",
    "\n",
    "## Instructions\n",
    "\n",
    "Total Points: 10\n",
    "\n",
    "Complete this notebook and submit it. The notebook needs to be a complete project report with \n",
    "\n",
    "* your implementation,\n",
    "* documentation including a short discussion of how your implementation works and your design choices, and\n",
    "* experimental results (e.g., tables and charts with simulation results) with a short discussion of what they mean. \n",
    "\n",
    "Use the provided notebook cells and insert additional code and markdown cells as needed.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "A spam detection agent gets as its percepts text messages and needs to decide if they are spam or not.\n",
    "Create a [naive Bayes classifier](https://en.wikipedia.org/wiki/Naive_Bayes_classifier) for the \n",
    "[UCI SMS Spam Collection Data Set](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection) to perform this task.\n",
    "\n",
    "__About the use of libraries:__ The point of this exercise is to learn how a Bayes classifier is built. You may use libraries for tokenizing, stop words and to create a document-term matrix, but you need to implement parameter estimation and prediction yourself.\n",
    "\n",
    "## Create a bag-of-words representation of the text messages [3 Points]\n",
    "\n",
    "The first step is to tokenize the text. Here is an example of how to use the [natural language tool kit (nltk)](https://www.nltk.org/) to create tokens (separate words)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with removing frequent words (called [stopwords](https://en.wikipedia.org/wiki/Stop_word)) and very infrequent words so you end up with a reasonable number of words used in the classifier. Maybe you need to remove digits or all non-letter characters. You may also use a stemming algorithm. \n",
    "\n",
    "Convert the tokenized data into a data structure that indicates for each for document what words it contains. The data structure can be a [document-term matrix](https://en.wikipedia.org/wiki/Document-term_matrix) with 0s and 1s, a pandas dataframe or some sparse matrix structure. Note: words, tokens and terms are often used interchangably. Make sure the data structure can be used to split the data into training and test documents (see below).\n",
    "\n",
    "Report the 20 most frequent and the 20 least frequent words in your data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "def print_full(x):\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', 2000)\n",
    "    pd.set_option('display.float_format', '{:20,.2f}'.format)\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    display(x)\n",
    "    pd.reset_option('display.max_rows')\n",
    "    pd.reset_option('display.max_columns')\n",
    "    pd.reset_option('display.width')\n",
    "    pd.reset_option('display.float_format')\n",
    "    pd.reset_option('display.max_colwidth')\n",
    "    \n",
    "def print_cm(cm, labels, hide_zeroes=False, hide_diagonal=False, hide_threshold=None):\n",
    "    \"\"\"\n",
    "    pretty print for confusion matrixes\n",
    "    taken from: https://gist.github.com/zachguo/10296432 \n",
    "    \"\"\"\n",
    "    columnwidth = max([len(x) for x in labels] + [5])  # 5 is value length\n",
    "    empty_cell = \" \" * columnwidth\n",
    "    \n",
    "    # Begin CHANGES\n",
    "    fst_empty_cell = (columnwidth-3)//2 * \" \" + \"t/p\" + (columnwidth-3)//2 * \" \"\n",
    "    \n",
    "    if len(fst_empty_cell) < len(empty_cell):\n",
    "        fst_empty_cell = \" \" * (len(empty_cell) - len(fst_empty_cell)) + fst_empty_cell\n",
    "    # Print header\n",
    "    print(\"    \" + fst_empty_cell, end=\" \")\n",
    "    # End CHANGES\n",
    "    \n",
    "    for label in labels:\n",
    "        print(\"%{0}s\".format(columnwidth) % label, end=\" \")\n",
    "        \n",
    "    print()\n",
    "    # Print rows\n",
    "    for i, label1 in enumerate(labels):\n",
    "        print(\"    %{0}s\".format(columnwidth) % label1, end=\" \")\n",
    "        for j in range(len(labels)):\n",
    "            cell = \"%{0}.1f\".format(columnwidth) % cm[i, j]\n",
    "            if hide_zeroes:\n",
    "                cell = cell if float(cm[i, j]) != 0 else empty_cell\n",
    "            if hide_diagonal:\n",
    "                cell = cell if i != j else empty_cell\n",
    "            if hide_threshold:\n",
    "                cell = cell if cm[i, j] > hide_threshold else empty_cell\n",
    "            print(cell, end=\" \")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of stopwords\n",
    "stopwords = [\"a\", \"about\", \"above\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\", \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\",\"although\",\"always\",\"am\",\"among\", \"amongst\", \"amoungst\", \"amount\",  \"an\", \"and\", \"another\", \"any\",\"anyhow\",\"anyone\",\"anything\",\"anyway\", \"anywhere\", \"are\", \"around\", \"as\",  \"at\", \"back\",\"be\",\"became\", \"because\",\"become\",\"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\", \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\", \"bottom\",\"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\", \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\", \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\",\"else\", \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\", \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\", \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\", \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"however\", \"hundred\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\", \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\", \"latterly\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\", \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\", \"never\", \"nevertheless\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\", \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\",\"part\", \"per\", \"perhaps\", \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\", \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\", \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thick\", \"thin\", \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\", \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\", \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\", \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\", \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\", \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"the\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import our dataset as pandas dataframe\n",
    "sms_spam = pd.read_csv('SMSSpamCollection', sep='\\t', header=None, names=['Class', 'Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Counts\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ham     0.86541\n",
       "spam    0.13459\n",
       "Name: Class, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Counts\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ham     0.868043\n",
       "spam    0.131957\n",
       "Name: Class, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# randomize the dataset\n",
    "data_randomized = sms_spam.sample(frac=1, random_state=1)\n",
    "\n",
    "# calculate index for split\n",
    "training_test_idx = round(len(data_randomized) * 0.8)\n",
    "\n",
    "# split into training and test sets\n",
    "# note: i clean the training data here but also have a function below  \n",
    "#   that cleans and classifies the test data as it analyzes it.\n",
    "train = data_randomized[:training_test_idx].reset_index(drop=True)\n",
    "test = data_randomized[training_test_idx:].reset_index(drop=True)\n",
    "\n",
    "# check counts\n",
    "print(\"Training Set Counts\")\n",
    "display(train['Class'].value_counts(normalize=True))\n",
    "print(\"Test Set Counts\")\n",
    "display(test['Class'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Text</th>\n",
       "      <th>Text_tok</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>yep  by the pretty sculpture</td>\n",
       "      <td>[yep, pretty, sculpture]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>yes  princess  are you going to make me moan</td>\n",
       "      <td>[yes, princess, going, make, moan]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>welp apparently he retired</td>\n",
       "      <td>[welp, apparently, retired]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>havent</td>\n",
       "      <td>[havent]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>i forgot 2 ask ü all smth   there s a card on ...</td>\n",
       "      <td>[forgot, ask, smth, card, da, present, lei, wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4453</th>\n",
       "      <td>ham</td>\n",
       "      <td>sorry  i ll call later in meeting any thing re...</td>\n",
       "      <td>[sorry, ll, later, meeting, thing, related, tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4454</th>\n",
       "      <td>ham</td>\n",
       "      <td>babe  i fucking love you too    you know  fuck...</td>\n",
       "      <td>[babe, fucking, love, know, fuck, good, hear, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4455</th>\n",
       "      <td>spam</td>\n",
       "      <td>u ve been selected to stay in 1 of 250 top bri...</td>\n",
       "      <td>[ve, selected, stay, british, hotels, holiday,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4456</th>\n",
       "      <td>ham</td>\n",
       "      <td>hello my boytoy     geeee i miss you already a...</td>\n",
       "      <td>[hello, boytoy, geeee, miss, just, woke, wish,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4457</th>\n",
       "      <td>ham</td>\n",
       "      <td>wherre s my boytoy</td>\n",
       "      <td>[wherre, boytoy]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4458 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Class                                               Text  \\\n",
       "0      ham                       yep  by the pretty sculpture   \n",
       "1      ham      yes  princess  are you going to make me moan    \n",
       "2      ham                         welp apparently he retired   \n",
       "3      ham                                            havent    \n",
       "4      ham  i forgot 2 ask ü all smth   there s a card on ...   \n",
       "...    ...                                                ...   \n",
       "4453   ham  sorry  i ll call later in meeting any thing re...   \n",
       "4454   ham  babe  i fucking love you too    you know  fuck...   \n",
       "4455  spam  u ve been selected to stay in 1 of 250 top bri...   \n",
       "4456   ham  hello my boytoy     geeee i miss you already a...   \n",
       "4457   ham                           wherre s my boytoy         \n",
       "\n",
       "                                               Text_tok  \n",
       "0                              [yep, pretty, sculpture]  \n",
       "1                    [yes, princess, going, make, moan]  \n",
       "2                           [welp, apparently, retired]  \n",
       "3                                              [havent]  \n",
       "4     [forgot, ask, smth, card, da, present, lei, wa...  \n",
       "...                                                 ...  \n",
       "4453  [sorry, ll, later, meeting, thing, related, tr...  \n",
       "4454  [babe, fucking, love, know, fuck, good, hear, ...  \n",
       "4455  [ve, selected, stay, british, hotels, holiday,...  \n",
       "4456  [hello, boytoy, geeee, miss, just, woke, wish,...  \n",
       "4457                                   [wherre, boytoy]  \n",
       "\n",
       "[4458 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# clean up the training set\n",
    "# remove punctuation\n",
    "train['Text'] = train['Text'].str.replace('\\W', ' ')\n",
    "# set all to lowercase\n",
    "train['Text'] = train['Text'].str.lower()\n",
    "# loop\n",
    "for i, row in train.iterrows():\n",
    "    # tokenize\n",
    "    tok = nltk.word_tokenize(row['Text'])\n",
    "    # remove not alpha characters\n",
    "    tok2 = [x for x in tok if x.isalpha()]\n",
    "    # remove 1 letter words\n",
    "    tok3 = [x for x in tok2 if len(x)>=2]\n",
    "    # remove stopwords and join\n",
    "    train.loc[i,'Text_tok'] = \" \".join(filter(lambda w: w not in stopwords, tok3))\n",
    "    # train.loc[i,'Text_tok'] = list(filter(lambda w: w not in stopwords, tok))\n",
    "    \n",
    "train['Text_tok'] = train['Text_tok'].str.split()\n",
    "display(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6487  unique terms in dataset.\n"
     ]
    }
   ],
   "source": [
    "# obtain list of all vocabulary\n",
    "vocab = []\n",
    "for text in train['Text_tok']:\n",
    "    for word in text:\n",
    "        vocab.append(word)\n",
    "\n",
    "vocab_counts = Counter(vocab)        \n",
    "vocab = list(set(vocab))\n",
    "print(len(vocab), \" unique terms in dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create one hot encoded train set\n",
    "text_counts = {unique_word: [0] * len(train['Text_tok']) for unique_word in vocab}\n",
    "for index, text in enumerate(train['Text_tok']):\n",
    "    for word in text:\n",
    "        text_counts[word][index] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>holla</th>\n",
       "      <th>hum</th>\n",
       "      <th>east</th>\n",
       "      <th>australia</th>\n",
       "      <th>permission</th>\n",
       "      <th>nos</th>\n",
       "      <th>influx</th>\n",
       "      <th>lodging</th>\n",
       "      <th>hockey</th>\n",
       "      <th>onluy</th>\n",
       "      <th>...</th>\n",
       "      <th>paracetamol</th>\n",
       "      <th>barcelona</th>\n",
       "      <th>neglet</th>\n",
       "      <th>waves</th>\n",
       "      <th>starting</th>\n",
       "      <th>wadebridge</th>\n",
       "      <th>date</th>\n",
       "      <th>virgins</th>\n",
       "      <th>svc</th>\n",
       "      <th>wat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 6487 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   holla  hum  east  australia  permission  nos  influx  lodging  hockey  \\\n",
       "0      0    0     0          0           0    0       0        0       0   \n",
       "1      0    0     0          0           0    0       0        0       0   \n",
       "2      0    0     0          0           0    0       0        0       0   \n",
       "3      0    0     0          0           0    0       0        0       0   \n",
       "4      0    0     0          0           0    0       0        0       0   \n",
       "\n",
       "   onluy  ...  paracetamol  barcelona  neglet  waves  starting  wadebridge  \\\n",
       "0      0  ...            0          0       0      0         0           0   \n",
       "1      0  ...            0          0       0      0         0           0   \n",
       "2      0  ...            0          0       0      0         0           0   \n",
       "3      0  ...            0          0       0      0         0           0   \n",
       "4      0  ...            0          0       0      0         0           0   \n",
       "\n",
       "   date  virgins  svc  wat  \n",
       "0     0        0    0    0  \n",
       "1     0        0    0    0  \n",
       "2     0        0    0    0  \n",
       "3     0        0    0    0  \n",
       "4     0        0    0    0  \n",
       "\n",
       "[5 rows x 6487 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert to a dataframe\n",
    "word_counts = pd.DataFrame(text_counts)\n",
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Text_tok</th>\n",
       "      <th>holla</th>\n",
       "      <th>hum</th>\n",
       "      <th>east</th>\n",
       "      <th>australia</th>\n",
       "      <th>permission</th>\n",
       "      <th>nos</th>\n",
       "      <th>influx</th>\n",
       "      <th>lodging</th>\n",
       "      <th>...</th>\n",
       "      <th>paracetamol</th>\n",
       "      <th>barcelona</th>\n",
       "      <th>neglet</th>\n",
       "      <th>waves</th>\n",
       "      <th>starting</th>\n",
       "      <th>wadebridge</th>\n",
       "      <th>date</th>\n",
       "      <th>virgins</th>\n",
       "      <th>svc</th>\n",
       "      <th>wat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>[yep, pretty, sculpture]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>[yes, princess, going, make, moan]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>[welp, apparently, retired]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>[havent]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>[forgot, ask, smth, card, da, present, lei, wa...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 6489 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Class                                           Text_tok  holla  hum  east  \\\n",
       "0   ham                           [yep, pretty, sculpture]      0    0     0   \n",
       "1   ham                 [yes, princess, going, make, moan]      0    0     0   \n",
       "2   ham                        [welp, apparently, retired]      0    0     0   \n",
       "3   ham                                           [havent]      0    0     0   \n",
       "4   ham  [forgot, ask, smth, card, da, present, lei, wa...      0    0     0   \n",
       "\n",
       "   australia  permission  nos  influx  lodging  ...  paracetamol  barcelona  \\\n",
       "0          0           0    0       0        0  ...            0          0   \n",
       "1          0           0    0       0        0  ...            0          0   \n",
       "2          0           0    0       0        0  ...            0          0   \n",
       "3          0           0    0       0        0  ...            0          0   \n",
       "4          0           0    0       0        0  ...            0          0   \n",
       "\n",
       "   neglet  waves  starting  wadebridge  date  virgins  svc  wat  \n",
       "0       0      0         0           0     0        0    0    0  \n",
       "1       0      0         0           0     0        0    0    0  \n",
       "2       0      0         0           0     0        0    0    0  \n",
       "3       0      0         0           0     0        0    0    0  \n",
       "4       0      0         0           0     0        0    0    0  \n",
       "\n",
       "[5 rows x 6489 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine old dataframe to new one-hot dataframe\n",
    "train_final = pd.concat([train, word_counts], axis=1)\n",
    "train_final = train_final.drop(['Text'], axis=1)\n",
    "train_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words in dataset with counts: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('ur', 316),\n",
       " ('just', 301),\n",
       " ('gt', 255),\n",
       " ('lt', 252),\n",
       " ('ok', 236),\n",
       " ('free', 223),\n",
       " ('ll', 221),\n",
       " ('know', 212),\n",
       " ('like', 196),\n",
       " ('good', 195),\n",
       " ('day', 194),\n",
       " ('come', 189),\n",
       " ('got', 184),\n",
       " ('time', 180),\n",
       " ('love', 172),\n",
       " ('send', 168),\n",
       " ('want', 154),\n",
       " ('text', 147),\n",
       " ('going', 146),\n",
       " ('txt', 142)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Most common words in dataset with counts: \")\n",
    "vocab_counts.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Least common words in dataset with counts: \n",
      "(Note that there were many words with a count of 1, these are just some)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('wherre', 1),\n",
       " ('arul', 1),\n",
       " ('trade', 1),\n",
       " ('related', 1),\n",
       " ('jewelry', 1),\n",
       " ('secrets', 1),\n",
       " ('hides', 1),\n",
       " ('beauty', 1),\n",
       " ('prakesh', 1),\n",
       " ('genes', 1),\n",
       " ('recession', 1),\n",
       " ('nag', 1),\n",
       " ('inconsiderate', 1),\n",
       " ('readers', 1),\n",
       " ('phoenix', 1),\n",
       " ('potter', 1),\n",
       " ('abbey', 1),\n",
       " ('swann', 1),\n",
       " ('armenia', 1),\n",
       " ('occupied', 1)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Least common words in dataset with counts: \")\n",
    "print(\"(Note that there were many words with a count of 1, these are just some)\")\n",
    "vocab_counts.most_common()[:-20-1:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn parameters [3 Points]\n",
    "\n",
    "Use 80% of the data (called training set; randomly chosen) to learn the parameters of the naive Bayes classifier (prior probabilities and likelihoods). \n",
    "Remember, the naive Bayes classifier calculates:\n",
    "\n",
    "$$P(spam|message) \\propto score_{spam}(message) = P(spam) \\prod_{i=1}^n P(w_i | spam)$$\n",
    "$$P(ham|message) \\propto score_{ham}(message) = P(ham) \\prod_{i=1}^n P(w_i | ham)$$\n",
    "\n",
    "and classifies a message as spam if \n",
    "$$score_{spam}(message) > score_{ham}(message).$$ \n",
    "\n",
    "You therefore need to\n",
    "estimate: \n",
    "\n",
    "* the priors $P(spam)$ and $P(ham)$, and \n",
    "* the likelihoods $P(w_i | spam)$ and $P(w_i | ham)$ for all words\n",
    "\n",
    "from counts obtained from the training data. Use [Laplacian smoothing](https://en.wikipedia.org/wiki/Additive_smoothing) to estimate the\n",
    "likelihoods. This deals with words that have very low count in the ham or spam messages and avoids\n",
    "likelihoods of zero.\n",
    "\n",
    "Report the top 20 words (highest conditional probability) for ham and for spam. These words represent the biggest clues that a message is ham or spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laplace smoothing\n",
    "alpha = 1\n",
    "# separate spam & ham\n",
    "spam = train_final[train_final['Class'] == 'spam']\n",
    "ham = train_final[train_final['Class'] == 'ham']\n",
    "# num vocab\n",
    "n_vocab = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(spam):  0.13458950201884254\n",
      "P(ham):  0.8654104979811574\n",
      "Length of all spam words:  7345\n",
      "Length of all ham words:  26855\n"
     ]
    }
   ],
   "source": [
    "# calc P(spam) and P(ham)\n",
    "p_spam = len(spam) / len(train_final)\n",
    "print(\"P(spam): \", p_spam)\n",
    "p_ham = len(ham) / len(train_final)\n",
    "print(\"P(ham): \", p_ham)\n",
    "\n",
    "# Length of all spam words\n",
    "num_words_spam = spam['Text_tok'].apply(len)\n",
    "n_spam = num_words_spam.sum()\n",
    "print(\"Length of all spam words: \", n_spam)\n",
    "\n",
    "# Length of all ham words\n",
    "num_words_ham = ham['Text_tok'].apply(len)\n",
    "n_ham = num_words_ham.sum()\n",
    "print(\"Length of all ham words: \", n_ham)\n",
    "\n",
    "# init P(w|spam) and P(w|ham)\n",
    "params_spam = {word:0 for word in vocab}\n",
    "params_ham = {word:0 for word in vocab}\n",
    "\n",
    "# calc P(w|spam) and P(w|ham)\n",
    "for word in vocab:\n",
    "    n_word_given_spam = spam[word].sum()\n",
    "    p_word_given_spam = (n_word_given_spam + alpha) / (n_spam + (alpha * n_vocab))\n",
    "    params_spam[word] = p_word_given_spam\n",
    "\n",
    "    n_word_given_ham = ham[word].sum()\n",
    "    p_word_given_ham = (n_word_given_ham + alpha) / (n_ham + (alpha * n_vocab))\n",
    "    params_ham[word] = p_word_given_ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Top ham words: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[('gt', 0.007678003719033051),\n",
       " ('lt', 0.007588027112950633),\n",
       " ('just', 0.0071681362845660125),\n",
       " ('ok', 0.006988183072401176),\n",
       " ('ll', 0.006568292244016556),\n",
       " ('ur', 0.006028432607522044),\n",
       " ('know', 0.005728510587247316),\n",
       " ('come', 0.005638533981164897),\n",
       " ('like', 0.005608541779137425),\n",
       " ('good', 0.005548557375082479),\n",
       " ('got', 0.005428588566972587),\n",
       " ('day', 0.0053086197588626954),\n",
       " ('time', 0.0049487133345330215),\n",
       " ('love', 0.004918721132505548),\n",
       " ('going', 0.004318877091956092),\n",
       " ('home', 0.004138923879791255),\n",
       " ('need', 0.004108931677763781),\n",
       " ('want', 0.0038989862635714716),\n",
       " ('sorry', 0.0038390018595165255),\n",
       " ('lor', 0.00377901745546158)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Top spam words: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[('free', 0.012724117987275883),\n",
       " ('txt', 0.009543088490456911),\n",
       " ('ur', 0.008458646616541353),\n",
       " ('stop', 0.0074465008675534995),\n",
       " ('mobile', 0.00708502024291498),\n",
       " ('text', 0.006795835743204164),\n",
       " ('claim', 0.006578947368421052),\n",
       " ('reply', 0.006434355118565645),\n",
       " ('www', 0.00578368999421631),\n",
       " ('prize', 0.005566801619433198),\n",
       " ('just', 0.004626951995373048),\n",
       " ('send', 0.004554655870445344),\n",
       " ('new', 0.004482359745517641),\n",
       " ('won', 0.004482359745517641),\n",
       " ('cash', 0.004410063620589937),\n",
       " ('nokia', 0.004265471370734529),\n",
       " ('uk', 0.004193175245806825),\n",
       " ('win', 0.0037593984962406013),\n",
       " ('tone', 0.003687102371312898),\n",
       " ('urgent', 0.003470213996529786)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "count = Counter(params_ham)\n",
    "display(\"Top ham words: \", count.most_common(20))\n",
    "count = Counter(params_spam)\n",
    "display(\"Top spam words: \", count.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to classify dataset\n",
    "def classify(message):\n",
    "    # remove punctuation\n",
    "    message = re.sub('\\W', ' ', message)\n",
    "    # lower case\n",
    "    message = message.lower()\n",
    "    # tokenize\n",
    "    message = nltk.word_tokenize(message)\n",
    "    # remove non alpha characters\n",
    "    message = [x for x in message if x.isalpha()]\n",
    "    # remove stopwords and make string\n",
    "    message = list(filter(lambda w: w not in stopwords, message))\n",
    "    # assign default probabilities\n",
    "    p_spam_given_message = p_spam\n",
    "    p_ham_given_message = p_ham\n",
    "    # loop through words in given message\n",
    "    for word in message:\n",
    "        # iteratively calculate score for spam\n",
    "        if word in params_spam:\n",
    "            p_spam_given_message *= params_spam[word]\n",
    "        # iteratively calculate score for ham\n",
    "        if word in params_ham:\n",
    "            p_ham_given_message *= params_ham[word]\n",
    "\n",
    "    if p_ham_given_message > p_spam_given_message:\n",
    "        return 'ham'\n",
    "    elif p_spam_given_message > p_ham_given_message:\n",
    "        return 'spam'\n",
    "    else:\n",
    "        return 'probabilities = 50%'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the classification performance [4 Points] \n",
    "\n",
    "Classify the remaining 20% of the data (test set) and calculate classification accuracy. Accuracy is defined as the proportion of correctly classified test documents.\n",
    "\n",
    "1. How good is your classifier's accuracy compared to a baseline classifier.\n",
    "\n",
    "2. Inspect a few misclassified text messages and discuss why the classification failed.\n",
    "\n",
    "3. Discuss how you deal with words in the test data that you have not seen in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Text</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Later i guess. I needa do mcat study too.</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>But i haf enuff space got like 4 mb...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Had your mobile 10 mths? Update to latest Oran...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>All sounds good. Fingers . Makes it difficult ...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>All done, all handed in. Don't know if mega sh...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Class                                               Text prediction\n",
       "0   ham          Later i guess. I needa do mcat study too.        ham\n",
       "1   ham             But i haf enuff space got like 4 mb...        ham\n",
       "2  spam  Had your mobile 10 mths? Update to latest Oran...       spam\n",
       "3   ham  All sounds good. Fingers . Makes it difficult ...        ham\n",
       "4   ham  All done, all handed in. Don't know if mega sh...        ham"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# store off prediction in test dataframe\n",
    "test_classify = test.copy()\n",
    "test_classify['prediction'] = test_classify['Text'].apply(classify)\n",
    "test_classify.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 1114\n",
      "Correct: 1090\n",
      "Incorrect: 24\n",
      "Accuracy: 97.84560143626571%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = test_classify.shape[0]\n",
    "# loop through dataframe to obtain number correct\n",
    "for row in test_classify.iterrows():\n",
    "    # if correct add 1 to correct var\n",
    "    if row[1]['Class'] == row[1]['prediction']:\n",
    "        correct += 1\n",
    "# print results\n",
    "print('Total:', total)\n",
    "print('Correct:', correct)\n",
    "print('Incorrect:', total - correct)\n",
    "print('Accuracy: {}%'.format((correct/total)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "     t/p    ham  spam \n",
      "      ham 953.0  14.0 \n",
      "     spam  10.0 137.0 \n",
      "\n",
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      0.99      0.99       967\n",
      "        spam       0.91      0.93      0.92       147\n",
      "\n",
      "    accuracy                           0.98      1114\n",
      "   macro avg       0.95      0.96      0.95      1114\n",
      "weighted avg       0.98      0.98      0.98      1114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(\"Confusion Matrix: \")\n",
    "print_cm(confusion_matrix(test_classify['Class'],test_classify['prediction']),['ham','spam'])\n",
    "print(\"\")\n",
    "print(\"Classification Report: \")\n",
    "print(classification_report(test_classify['Class'],test_classify['prediction']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. <b>How good is your classifier's accuracy compared to a baseline classifier.</b>\n",
    "\n",
    "As seen above, my classifier has an accuracy of 97.85%. Out of 1114 text messages of test data it correctly classified 1090 of the messages and only incorrectly classified 24 of the messages. Compared to a baseline classifier I took a look at the research paper https://pdfs.semanticscholar.org/22de/da99e2c06d497c41e1b7a45c9e920c0367f2.pdf and saw that they were able to obtain an average accuracy of 97.4%, which is a bit less performant than mine. Additionally, my classsifier was most definitely better than a weak baseline of always predicting 'spam' or 'ham', which at best would have had an accuracy of 86.8% if predicted all ham.\n",
    "\n",
    "Furthermore, taking a look above at the confusion matrix above, our false negative count (# of messages that were spam but were classified as legit) was 14 which is pretty low along with the true negative count (# of messages that were legit but classified as spam) being 10 which is even lower. Given the size of the dataset these numbers are quite good. Additionally, in our classification report we can see that our recall and precision scores are really good. This is important as recall determines the proportion of legitimate messages which have been correctly categorized while precision determines the proportion of all correctly categorized messages which are legitimate.\n",
    "\n",
    "Given these results I would argue that the classifier performed quite well! I of course anticipate that more advanced ML algorithms out there would perform better but for a naive bayes classifier, this one worked quite well!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Text</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ham</td>\n",
       "      <td>I liked the new mobile</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>ham</td>\n",
       "      <td>How was txting and driving</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>ham</td>\n",
       "      <td>Unlimited texts. Limited minutes.</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>ham</td>\n",
       "      <td>26th OF JULY</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nokia phone is lovly..</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>ham</td>\n",
       "      <td>No calls..messages..missed calls</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>ham</td>\n",
       "      <td>We have sent JD for Customer Service cum Accounts Executive to ur mail id, For details contact us</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>ham</td>\n",
       "      <td>8 at the latest, g's still there if you can scrounge up some ammo and want to give the new ak a try</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>ham</td>\n",
       "      <td>Hasn't that been the pattern recently crap weekends?</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>660</th>\n",
       "      <td>ham</td>\n",
       "      <td>Any chance you might have had with me evaporated as soon as you violated my privacy by stealing my phone number from your employer's paperwork. Not cool at all. Please do not contact me again or I will report you to your supervisor.</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>ham</td>\n",
       "      <td>Can... I'm free...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>ham</td>\n",
       "      <td>These won't do. Have to move on to morphine</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923</th>\n",
       "      <td>ham</td>\n",
       "      <td>How much would it cost to hire a hitman</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1113</th>\n",
       "      <td>ham</td>\n",
       "      <td>K k:) sms chat with me.</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Class                                                                                                                                                                                                                                      Text prediction\n",
       "9      ham                                                                                                                                                                                                                    I liked the new mobile       spam\n",
       "130    ham                                                                                                                                                                                                                How was txting and driving       spam\n",
       "152    ham                                                                                                                                                                                                         Unlimited texts. Limited minutes.       spam\n",
       "159    ham                                                                                                                                                                                                                              26th OF JULY       spam\n",
       "284    ham                                                                                                                                                                                                                    Nokia phone is lovly..       spam\n",
       "302    ham                                                                                                                                                                                                          No calls..messages..missed calls       spam\n",
       "319    ham                                                                                                                                         We have sent JD for Customer Service cum Accounts Executive to ur mail id, For details contact us       spam\n",
       "361    ham                                                                                                                                       8 at the latest, g's still there if you can scrounge up some ammo and want to give the new ak a try       spam\n",
       "398    ham                                                                                                                                                                                      Hasn't that been the pattern recently crap weekends?       spam\n",
       "660    ham  Any chance you might have had with me evaporated as soon as you violated my privacy by stealing my phone number from your employer's paperwork. Not cool at all. Please do not contact me again or I will report you to your supervisor.       spam\n",
       "779    ham                                                                                                                                                                                                                        Can... I'm free...       spam\n",
       "824    ham                                                                                                                                                                                               These won't do. Have to move on to morphine       spam\n",
       "923    ham                                                                                                                                                                                                   How much would it cost to hire a hitman       spam\n",
       "1113   ham                                                                                                                                                                                                                   K k:) sms chat with me.       spam"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Text</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>spam</td>\n",
       "      <td>Not heard from U4 a while. Call me now am here all night with just my knickers on. Make me beg for it like U did last time 01223585236 XX Luv Nikiyu4.net</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>spam</td>\n",
       "      <td>More people are dogging in your area now. Call 09090204448 and join like minded guys. Why not arrange 1 yourself. There's 1 this evening. A£1.50 minAPN LS278BB</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>spam</td>\n",
       "      <td>Email AlertFrom: Jeri StewartSize: 2KBSubject: Low-cost prescripiton drvgsTo listen to email call 123</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>spam</td>\n",
       "      <td>Ur balance is now £600. Next question: Complete the landmark, Big, A. Bob, B. Barry or C. Ben ?. Text A, B or C to 83738. Good luck!</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>spam</td>\n",
       "      <td>Oh my god! I've found your number again! I'm so glad, text me back xafter this msgs cst std ntwk chg £1.50</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604</th>\n",
       "      <td>spam</td>\n",
       "      <td>88066 FROM 88066 LOST 3POUND HELP</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>spam</td>\n",
       "      <td>Burger King - Wanna play footy at a top stadium? Get 2 Burger King before 1st Sept and go Large or Super with Coca-Cola and walk out a winner</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>spam</td>\n",
       "      <td>RCT' THNQ Adrian for U text. Rgds Vatian</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>spam</td>\n",
       "      <td>2/2 146tf150p</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>spam</td>\n",
       "      <td>Hello. We need some posh birds and chaps to user trial prods for champneys. Can i put you down? I need your address and dob asap. Ta r</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Class                                                                                                                                                             Text prediction\n",
       "114  spam        Not heard from U4 a while. Call me now am here all night with just my knickers on. Make me beg for it like U did last time 01223585236 XX Luv Nikiyu4.net        ham\n",
       "135  spam  More people are dogging in your area now. Call 09090204448 and join like minded guys. Why not arrange 1 yourself. There's 1 this evening. A£1.50 minAPN LS278BB        ham\n",
       "363  spam                                                            Email AlertFrom: Jeri StewartSize: 2KBSubject: Low-cost prescripiton drvgsTo listen to email call 123        ham\n",
       "500  spam                             Ur balance is now £600. Next question: Complete the landmark, Big, A. Bob, B. Barry or C. Ben ?. Text A, B or C to 83738. Good luck!        ham\n",
       "504  spam                                                       Oh my god! I've found your number again! I'm so glad, text me back xafter this msgs cst std ntwk chg £1.50        ham\n",
       "604  spam                                                                                                                                88066 FROM 88066 LOST 3POUND HELP        ham\n",
       "673  spam                    Burger King - Wanna play footy at a top stadium? Get 2 Burger King before 1st Sept and go Large or Super with Coca-Cola and walk out a winner        ham\n",
       "876  spam                                                                                                                         RCT' THNQ Adrian for U text. Rgds Vatian        ham\n",
       "885  spam                                                                                                                                                    2/2 146tf150p        ham\n",
       "953  spam                           Hello. We need some posh birds and chaps to user trial prods for champneys. Can i put you down? I need your address and dob asap. Ta r        ham"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wrong_spam = pd.DataFrame()\n",
    "wrong_ham = pd.DataFrame()\n",
    "for row in test_classify.iterrows():\n",
    "    if row[1]['Class'] != row[1]['prediction']:\n",
    "        if(row[1]['Class'] == 'ham'):\n",
    "            wrong_ham = wrong_ham.append(row[1])\n",
    "        if(row[1]['Class'] == 'spam'):\n",
    "            wrong_spam = wrong_spam.append(row[1])\n",
    "\n",
    "print_full(wrong_ham)\n",
    "print_full(wrong_spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. <b>Inspect a few misclassified text messages and discuss why the classification failed.</b>\n",
    "\n",
    "Above you can see all of the misclassified messages. To make it a bit easier, let us first clean the data so we can see what the classifier saw and drop the class and prediction columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(df_in):\n",
    "    df = df_in.copy()\n",
    "    # remove punctuation\n",
    "    df['Text'] = df['Text'].str.replace('\\W', ' ')\n",
    "    # set all to lowercase\n",
    "    df['Text'] = df['Text'].str.lower()\n",
    "    # loop\n",
    "    for i, row in df.iterrows():\n",
    "        # tokenize\n",
    "        tok = nltk.word_tokenize(row['Text'])\n",
    "        # remove not alpha characters\n",
    "        tok2 = [x for x in tok if x.isalpha()]\n",
    "        # remove 1 letter words\n",
    "        tok3 = [x for x in tok2 if len(x)>=2]\n",
    "        # remove stopwords and join\n",
    "        df.loc[i,'Text'] = \" \".join(filter(lambda w: w not in stopwords, tok3))\n",
    "    return df\n",
    "\n",
    "wrong_ham = wrong_ham.drop(['Class', 'prediction'], axis=1)\n",
    "wrong_spam = wrong_spam.drop(['Class', 'prediction'],axis=1)\n",
    "\n",
    "wrong_ham_clean = clean(wrong_ham)\n",
    "wrong_spam_clean = clean(wrong_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I liked the new mobile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>How was txting and driving</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>Unlimited texts. Limited minutes.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>26th OF JULY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>Nokia phone is lovly..</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>No calls..messages..missed calls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>We have sent JD for Customer Service cum Accounts Executive to ur mail id, For details contact us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>8 at the latest, g's still there if you can scrounge up some ammo and want to give the new ak a try</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>Hasn't that been the pattern recently crap weekends?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>660</th>\n",
       "      <td>Any chance you might have had with me evaporated as soon as you violated my privacy by stealing my phone number from your employer's paperwork. Not cool at all. Please do not contact me again or I will report you to your supervisor.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>Can... I'm free...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>These won't do. Have to move on to morphine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923</th>\n",
       "      <td>How much would it cost to hire a hitman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1113</th>\n",
       "      <td>K k:) sms chat with me.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                          Text\n",
       "9                                                                                                                                                                                                                       I liked the new mobile\n",
       "130                                                                                                                                                                                                                 How was txting and driving\n",
       "152                                                                                                                                                                                                          Unlimited texts. Limited minutes.\n",
       "159                                                                                                                                                                                                                               26th OF JULY\n",
       "284                                                                                                                                                                                                                     Nokia phone is lovly..\n",
       "302                                                                                                                                                                                                           No calls..messages..missed calls\n",
       "319                                                                                                                                          We have sent JD for Customer Service cum Accounts Executive to ur mail id, For details contact us\n",
       "361                                                                                                                                        8 at the latest, g's still there if you can scrounge up some ammo and want to give the new ak a try\n",
       "398                                                                                                                                                                                       Hasn't that been the pattern recently crap weekends?\n",
       "660   Any chance you might have had with me evaporated as soon as you violated my privacy by stealing my phone number from your employer's paperwork. Not cool at all. Please do not contact me again or I will report you to your supervisor.\n",
       "779                                                                                                                                                                                                                         Can... I'm free...\n",
       "824                                                                                                                                                                                                These won't do. Have to move on to morphine\n",
       "923                                                                                                                                                                                                    How much would it cost to hire a hitman\n",
       "1113                                                                                                                                                                                                                   K k:) sms chat with me."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>liked new mobile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>txting driving</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>unlimited texts limited minutes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>july</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>nokia phone lovly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>calls messages missed calls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>sent jd customer service cum accounts executive ur mail id details contact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>latest scrounge ammo want new ak try</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>hasn pattern recently crap weekends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>660</th>\n",
       "      <td>chance evaporated soon violated privacy stealing phone number employer paperwork cool contact report supervisor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>free</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>won morphine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923</th>\n",
       "      <td>cost hire hitman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1113</th>\n",
       "      <td>sms chat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                 Text\n",
       "9                                                                                                    liked new mobile\n",
       "130                                                                                                    txting driving\n",
       "152                                                                                   unlimited texts limited minutes\n",
       "159                                                                                                              july\n",
       "284                                                                                                 nokia phone lovly\n",
       "302                                                                                       calls messages missed calls\n",
       "319                                        sent jd customer service cum accounts executive ur mail id details contact\n",
       "361                                                                              latest scrounge ammo want new ak try\n",
       "398                                                                               hasn pattern recently crap weekends\n",
       "660   chance evaporated soon violated privacy stealing phone number employer paperwork cool contact report supervisor\n",
       "779                                                                                                              free\n",
       "824                                                                                                      won morphine\n",
       "923                                                                                                  cost hire hitman\n",
       "1113                                                                                                         sms chat"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_full(wrong_ham)\n",
    "print_full(wrong_ham_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we take a look at some of the instances where the messages were classified as spam but were ham (above), we can see that they typically contain many spelling errors or contain words pertaining to phone calls/texts or business related situations. Additionally, some of these words may have been used quite infrequently and when they were used were part of a spam message. This would explain it as we see above, alot of the words like 'hire', 'sms', 'unlimited', 'texts', and 'calls' are seen quite a bit in spam messages since they often try to scam you. Removing infrequent words here could help but I felt like it might make the classifier not work as well due to the size of the dataset. Additionally, it should be noted as stated below for question 3 that words not seen in the training equate to nothing when classifying. This could also be another reason why it is failing when it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>Not heard from U4 a while. Call me now am here all night with just my knickers on. Make me beg for it like U did last time 01223585236 XX Luv Nikiyu4.net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>More people are dogging in your area now. Call 09090204448 and join like minded guys. Why not arrange 1 yourself. There's 1 this evening. A£1.50 minAPN LS278BB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>Email AlertFrom: Jeri StewartSize: 2KBSubject: Low-cost prescripiton drvgsTo listen to email call 123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>Ur balance is now £600. Next question: Complete the landmark, Big, A. Bob, B. Barry or C. Ben ?. Text A, B or C to 83738. Good luck!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>Oh my god! I've found your number again! I'm so glad, text me back xafter this msgs cst std ntwk chg £1.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604</th>\n",
       "      <td>88066 FROM 88066 LOST 3POUND HELP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>Burger King - Wanna play footy at a top stadium? Get 2 Burger King before 1st Sept and go Large or Super with Coca-Cola and walk out a winner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>RCT' THNQ Adrian for U text. Rgds Vatian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>2/2 146tf150p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>Hello. We need some posh birds and chaps to user trial prods for champneys. Can i put you down? I need your address and dob asap. Ta r</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                Text\n",
       "114        Not heard from U4 a while. Call me now am here all night with just my knickers on. Make me beg for it like U did last time 01223585236 XX Luv Nikiyu4.net\n",
       "135  More people are dogging in your area now. Call 09090204448 and join like minded guys. Why not arrange 1 yourself. There's 1 this evening. A£1.50 minAPN LS278BB\n",
       "363                                                            Email AlertFrom: Jeri StewartSize: 2KBSubject: Low-cost prescripiton drvgsTo listen to email call 123\n",
       "500                             Ur balance is now £600. Next question: Complete the landmark, Big, A. Bob, B. Barry or C. Ben ?. Text A, B or C to 83738. Good luck!\n",
       "504                                                       Oh my god! I've found your number again! I'm so glad, text me back xafter this msgs cst std ntwk chg £1.50\n",
       "604                                                                                                                                88066 FROM 88066 LOST 3POUND HELP\n",
       "673                    Burger King - Wanna play footy at a top stadium? Get 2 Burger King before 1st Sept and go Large or Super with Coca-Cola and walk out a winner\n",
       "876                                                                                                                         RCT' THNQ Adrian for U text. Rgds Vatian\n",
       "885                                                                                                                                                    2/2 146tf150p\n",
       "953                           Hello. We need some posh birds and chaps to user trial prods for champneys. Can i put you down? I need your address and dob asap. Ta r"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>heard night just knickers make beg like did time xx luv net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>people dogging area join like minded guys arrange evening minapn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>email alertfrom jeri stewartsize low cost prescripiton drvgsto listen email</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>ur balance question complete landmark big bob barry ben text good luck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>oh god ve number glad text xafter msgs cst std ntwk chg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604</th>\n",
       "      <td>lost help</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>burger king wan na play footy stadium burger king sept large super coca cola walk winner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>rct thnq adrian text rgds vatian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>hello need posh birds chaps user trial prods champneys need address dob asap ta</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                         Text\n",
       "114                               heard night just knickers make beg like did time xx luv net\n",
       "135                          people dogging area join like minded guys arrange evening minapn\n",
       "363               email alertfrom jeri stewartsize low cost prescripiton drvgsto listen email\n",
       "500                    ur balance question complete landmark big bob barry ben text good luck\n",
       "504                                   oh god ve number glad text xafter msgs cst std ntwk chg\n",
       "604                                                                                 lost help\n",
       "673  burger king wan na play footy stadium burger king sept large super coca cola walk winner\n",
       "876                                                          rct thnq adrian text rgds vatian\n",
       "885                                                                                          \n",
       "953           hello need posh birds chaps user trial prods champneys need address dob asap ta"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_full(wrong_spam)\n",
    "print_full(wrong_spam_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we take a look at some of the instances where messages were classified as ham but were spam (above), we can see that these messages drastically change after going through the cleaning process and that it kind of makes sense they were designated as non spam. A lot of the numbers and symbols were also elmintated which probably would have helped if they could have been included, but this would make our classifier larger and less compact. For the most part there are not many words that really scream spam after the cleaning and if there are there is a chance that, like stated above, they were not in the training data set and therefore equate to nothing during classification. Like above, removing infrequent words may help and we will give that a try down below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. <b>Discuss how you deal with words in the test data that you have not seen in the training data.<b>\n",
    "\n",
    "For words that were not seen during training and are not apart of the vocabulary, they are simply omitted from the probability scoring during classification. I felt it was best to do it this way for the purposes of this project but stemming could be used for words that are slightly different but carry the same meaning. This would allow for more words in a given query to be counted since they wouldnt have to be identical to the words in the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus task [+1 Point]\n",
    "\n",
    "Describe how you could improve the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I believe that I could improve the classifier by simply removing some of the infrequent words that have only a count of one. Additonally, I feel like stemming words could have improved the classifier, this would in turn increase the reach that the vocabulary list had on words with different endings (-ly, -ing, etc.). Also, incorporating numbers in a way that made them meaningful would have probably helped as well. However, just to see how it performs, let's take a look at removing infrequent words with a count <= 2 below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Counts\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ham     0.86541\n",
       "spam    0.13459\n",
       "Name: Class, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Counts\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ham     0.868043\n",
       "spam    0.131957\n",
       "Name: Class, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# randomize the dataset\n",
    "data_randomized2 = sms_spam.sample(frac=1, random_state=1)\n",
    "\n",
    "# calculate index for split\n",
    "training_test_idx = round(len(data_randomized2) * 0.8)\n",
    "\n",
    "# split into training and test sets\n",
    "# note: i clean the training data here but also have a function below  \n",
    "#   that cleans and classifies the test data as it analyzes it.\n",
    "train2 = data_randomized[:training_test_idx].reset_index(drop=True)\n",
    "test2 = data_randomized[training_test_idx:].reset_index(drop=True)\n",
    "\n",
    "# check counts\n",
    "print(\"Training Set Counts\")\n",
    "display(train2['Class'].value_counts(normalize=True))\n",
    "print(\"Test Set Counts\")\n",
    "display(test2['Class'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Text</th>\n",
       "      <th>Text_tok</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>yep  by the pretty sculpture</td>\n",
       "      <td>[yep, pretty, sculpture]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>yes  princess  are you going to make me moan</td>\n",
       "      <td>[yes, princess, going, make, moan]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>welp apparently he retired</td>\n",
       "      <td>[welp, apparently, retired]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>havent</td>\n",
       "      <td>[havent]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>i forgot 2 ask ü all smth   there s a card on ...</td>\n",
       "      <td>[forgot, ask, smth, card, da, present, lei, wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4453</th>\n",
       "      <td>ham</td>\n",
       "      <td>sorry  i ll call later in meeting any thing re...</td>\n",
       "      <td>[sorry, ll, later, meeting, thing, related, tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4454</th>\n",
       "      <td>ham</td>\n",
       "      <td>babe  i fucking love you too    you know  fuck...</td>\n",
       "      <td>[babe, fucking, love, know, fuck, good, hear, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4455</th>\n",
       "      <td>spam</td>\n",
       "      <td>u ve been selected to stay in 1 of 250 top bri...</td>\n",
       "      <td>[ve, selected, stay, british, hotels, holiday,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4456</th>\n",
       "      <td>ham</td>\n",
       "      <td>hello my boytoy     geeee i miss you already a...</td>\n",
       "      <td>[hello, boytoy, geeee, miss, just, woke, wish,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4457</th>\n",
       "      <td>ham</td>\n",
       "      <td>wherre s my boytoy</td>\n",
       "      <td>[wherre, boytoy]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4458 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Class                                               Text  \\\n",
       "0      ham                       yep  by the pretty sculpture   \n",
       "1      ham      yes  princess  are you going to make me moan    \n",
       "2      ham                         welp apparently he retired   \n",
       "3      ham                                            havent    \n",
       "4      ham  i forgot 2 ask ü all smth   there s a card on ...   \n",
       "...    ...                                                ...   \n",
       "4453   ham  sorry  i ll call later in meeting any thing re...   \n",
       "4454   ham  babe  i fucking love you too    you know  fuck...   \n",
       "4455  spam  u ve been selected to stay in 1 of 250 top bri...   \n",
       "4456   ham  hello my boytoy     geeee i miss you already a...   \n",
       "4457   ham                           wherre s my boytoy         \n",
       "\n",
       "                                               Text_tok  \n",
       "0                              [yep, pretty, sculpture]  \n",
       "1                    [yes, princess, going, make, moan]  \n",
       "2                           [welp, apparently, retired]  \n",
       "3                                              [havent]  \n",
       "4     [forgot, ask, smth, card, da, present, lei, wa...  \n",
       "...                                                 ...  \n",
       "4453  [sorry, ll, later, meeting, thing, related, tr...  \n",
       "4454  [babe, fucking, love, know, fuck, good, hear, ...  \n",
       "4455  [ve, selected, stay, british, hotels, holiday,...  \n",
       "4456  [hello, boytoy, geeee, miss, just, woke, wish,...  \n",
       "4457                                   [wherre, boytoy]  \n",
       "\n",
       "[4458 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# clean up the training set\n",
    "# remove punctuation\n",
    "train2['Text'] = train2['Text'].str.replace('\\W', ' ')\n",
    "# set all to lowercase\n",
    "train2['Text'] = train2['Text'].str.lower()\n",
    "# loop\n",
    "for i, row in train2.iterrows():\n",
    "    # tokenize\n",
    "    tok = nltk.word_tokenize(row['Text'])\n",
    "    # remove not alpha characters\n",
    "    tok2 = [x for x in tok if x.isalpha()]\n",
    "    # remove 1 letter words\n",
    "    tok3 = [x for x in tok2 if len(x)>=2]\n",
    "    # remove stopwords and join\n",
    "    train2.loc[i,'Text_tok'] = \" \".join(filter(lambda w: w not in stopwords, tok3))\n",
    "    # train.loc[i,'Text_tok'] = list(filter(lambda w: w not in stopwords, tok))\n",
    "    \n",
    "train2['Text_tok'] = train2['Text_tok'].str.split()\n",
    "display(train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unqiue vocab words:  6487\n"
     ]
    }
   ],
   "source": [
    "# obtain list of all vocabulary\n",
    "vocab2 = []\n",
    "unique_vocab = {}\n",
    "for text in train2['Text_tok']:\n",
    "    for word in text:\n",
    "        vocab2.append(word)\n",
    "\n",
    "for c in range(len(vocab2)):\n",
    "    unique_vocab[vocab2[c]] = vocab2.count(vocab2[c])\n",
    "\n",
    "print(\"Number of unqiue vocab words: \", len(unique_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unqiue vocab words occuring >= 2 times:  3060\n"
     ]
    }
   ],
   "source": [
    "for k,v in list(unique_vocab.items()):\n",
    "    if v <= 1:\n",
    "        del unique_vocab[k]\n",
    "print(\"Number of unqiue vocab words occuring >= 2 times: \", len(unique_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create one hot encoded train set\n",
    "text_counts2 = {unique_word: [0] * len(train2['Text_tok']) for unique_word in list(unique_vocab.keys())}\n",
    "for index, text in enumerate(train2['Text_tok']):\n",
    "    for word in text:\n",
    "        if(word in set(unique_vocab.keys())):\n",
    "            text_counts2[word][index] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>yep</th>\n",
       "      <th>pretty</th>\n",
       "      <th>yes</th>\n",
       "      <th>princess</th>\n",
       "      <th>going</th>\n",
       "      <th>make</th>\n",
       "      <th>moan</th>\n",
       "      <th>welp</th>\n",
       "      <th>apparently</th>\n",
       "      <th>havent</th>\n",
       "      <th>...</th>\n",
       "      <th>lyfu</th>\n",
       "      <th>lyf</th>\n",
       "      <th>ali</th>\n",
       "      <th>ke</th>\n",
       "      <th>meow</th>\n",
       "      <th>favorite</th>\n",
       "      <th>pap</th>\n",
       "      <th>rhythm</th>\n",
       "      <th>cme</th>\n",
       "      <th>harry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3060 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   yep  pretty  yes  princess  going  make  moan  welp  apparently  havent  \\\n",
       "0    1       1    0         0      0     0     0     0           0       0   \n",
       "1    0       0    1         1      1     1     1     0           0       0   \n",
       "2    0       0    0         0      0     0     0     1           1       0   \n",
       "3    0       0    0         0      0     0     0     0           0       1   \n",
       "4    0       0    0         0      0     0     0     0           0       0   \n",
       "\n",
       "   ...  lyfu  lyf  ali  ke  meow  favorite  pap  rhythm  cme  harry  \n",
       "0  ...     0    0    0   0     0         0    0       0    0      0  \n",
       "1  ...     0    0    0   0     0         0    0       0    0      0  \n",
       "2  ...     0    0    0   0     0         0    0       0    0      0  \n",
       "3  ...     0    0    0   0     0         0    0       0    0      0  \n",
       "4  ...     0    0    0   0     0         0    0       0    0      0  \n",
       "\n",
       "[5 rows x 3060 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert to a dataframe\n",
    "word_counts2 = pd.DataFrame(text_counts2)\n",
    "word_counts2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Text_tok</th>\n",
       "      <th>yep</th>\n",
       "      <th>pretty</th>\n",
       "      <th>yes</th>\n",
       "      <th>princess</th>\n",
       "      <th>going</th>\n",
       "      <th>make</th>\n",
       "      <th>moan</th>\n",
       "      <th>welp</th>\n",
       "      <th>...</th>\n",
       "      <th>lyfu</th>\n",
       "      <th>lyf</th>\n",
       "      <th>ali</th>\n",
       "      <th>ke</th>\n",
       "      <th>meow</th>\n",
       "      <th>favorite</th>\n",
       "      <th>pap</th>\n",
       "      <th>rhythm</th>\n",
       "      <th>cme</th>\n",
       "      <th>harry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>[yep, pretty, sculpture]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>[yes, princess, going, make, moan]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>[welp, apparently, retired]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>[havent]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>[forgot, ask, smth, card, da, present, lei, wa...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3062 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Class                                           Text_tok  yep  pretty  yes  \\\n",
       "0   ham                           [yep, pretty, sculpture]    1       1    0   \n",
       "1   ham                 [yes, princess, going, make, moan]    0       0    1   \n",
       "2   ham                        [welp, apparently, retired]    0       0    0   \n",
       "3   ham                                           [havent]    0       0    0   \n",
       "4   ham  [forgot, ask, smth, card, da, present, lei, wa...    0       0    0   \n",
       "\n",
       "   princess  going  make  moan  welp  ...  lyfu  lyf  ali  ke  meow  favorite  \\\n",
       "0         0      0     0     0     0  ...     0    0    0   0     0         0   \n",
       "1         1      1     1     1     0  ...     0    0    0   0     0         0   \n",
       "2         0      0     0     0     1  ...     0    0    0   0     0         0   \n",
       "3         0      0     0     0     0  ...     0    0    0   0     0         0   \n",
       "4         0      0     0     0     0  ...     0    0    0   0     0         0   \n",
       "\n",
       "   pap  rhythm  cme  harry  \n",
       "0    0       0    0      0  \n",
       "1    0       0    0      0  \n",
       "2    0       0    0      0  \n",
       "3    0       0    0      0  \n",
       "4    0       0    0      0  \n",
       "\n",
       "[5 rows x 3062 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine old dataframe to new one-hot dataframe\n",
    "train_final2 = pd.concat([train2, word_counts2], axis=1)\n",
    "train_final2 = train_final2.drop(['Text'], axis=1)\n",
    "train_final2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laplace smoothing\n",
    "alpha = 1\n",
    "# separate spam & ham\n",
    "spam2 = train_final2[train_final2['Class'] == 'spam']\n",
    "ham2 = train_final2[train_final2['Class'] == 'ham']\n",
    "# num vocab\n",
    "n_vocab2 = len(unique_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(spam):  0.13458950201884254\n",
      "P(ham):  0.8654104979811574\n",
      "Length of all spam words:  7345\n",
      "Length of all ham words:  26855\n"
     ]
    }
   ],
   "source": [
    "# calc P(spam) and P(ham)\n",
    "p_spam2 = len(spam2) / len(train_final2)\n",
    "print(\"P(spam): \", p_spam2)\n",
    "p_ham2 = len(ham2) / len(train_final2)\n",
    "print(\"P(ham): \", p_ham2)\n",
    "\n",
    "# Length of all spam words\n",
    "num_words_spam2 = spam2['Text_tok'].apply(len)\n",
    "n_spam2 = num_words_spam2.sum()\n",
    "print(\"Length of all spam words: \", n_spam2)\n",
    "\n",
    "# Length of all ham words\n",
    "num_words_ham2 = ham2['Text_tok'].apply(len)\n",
    "n_ham2 = num_words_ham2.sum()\n",
    "print(\"Length of all ham words: \", n_ham2)\n",
    "\n",
    "# init P(w|spam) and P(w|ham)\n",
    "params_spam2 = {word:0 for word in unique_vocab}\n",
    "params_ham2 = {word:0 for word in unique_vocab}\n",
    "\n",
    "# calc P(w|spam) and P(w|ham)\n",
    "for word in unique_vocab:\n",
    "    n_word_given_spam2 = spam2[word].sum()\n",
    "    p_word_given_spam2 = (n_word_given_spam2 + alpha) / (n_spam2 + (alpha * n_vocab2))\n",
    "    params_spam2[word] = p_word_given_spam2\n",
    "\n",
    "    n_word_given_ham2 = ham2[word].sum()\n",
    "    p_word_given_ham2 = (n_word_given_ham2 + alpha) / (n_ham2 + (alpha * n_vocab2))\n",
    "    params_ham2[word] = p_word_given_ham2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to classify dataset\n",
    "def classify2(message):\n",
    "    # remove punctuation\n",
    "    message = re.sub('\\W', ' ', message)\n",
    "    # lower case\n",
    "    message = message.lower()\n",
    "    # tokenize\n",
    "    message = nltk.word_tokenize(message)\n",
    "    # remove non alpha characters\n",
    "    message = [x for x in message if x.isalpha()]\n",
    "    # remove stopwords and make string\n",
    "    message = list(filter(lambda w: w not in stopwords, message))\n",
    "    # assign default probabilities\n",
    "    p_spam_given_message2 = p_spam2\n",
    "    p_ham_given_message2 = p_ham2\n",
    "    # loop through words in given message\n",
    "    for word in message:\n",
    "        # iteratively calculate score for spam\n",
    "        if word in params_spam2:\n",
    "            p_spam_given_message2 *= params_spam2[word]\n",
    "        # iteratively calculate score for ham\n",
    "        if word in params_ham2:\n",
    "            p_ham_given_message2 *= params_ham2[word]\n",
    "\n",
    "    if p_ham_given_message2 > p_spam_given_message2:\n",
    "        return 'ham'\n",
    "    elif p_spam_given_message2 > p_ham_given_message2:\n",
    "        return 'spam'\n",
    "    else:\n",
    "        return 'probabilities = 50%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Text</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Later i guess. I needa do mcat study too.</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>But i haf enuff space got like 4 mb...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Had your mobile 10 mths? Update to latest Oran...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>All sounds good. Fingers . Makes it difficult ...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>All done, all handed in. Don't know if mega sh...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Class                                               Text prediction\n",
       "0   ham          Later i guess. I needa do mcat study too.        ham\n",
       "1   ham             But i haf enuff space got like 4 mb...        ham\n",
       "2  spam  Had your mobile 10 mths? Update to latest Oran...       spam\n",
       "3   ham  All sounds good. Fingers . Makes it difficult ...        ham\n",
       "4   ham  All done, all handed in. Don't know if mega sh...        ham"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# store off prediction in test dataframe\n",
    "test_classify2 = test2.copy()\n",
    "test_classify2['prediction'] = test_classify2['Text'].apply(classify2)\n",
    "test_classify2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 1114\n",
      "Correct: 1086\n",
      "Incorrect: 28\n",
      "Accuracy: 97.48653500897666%\n"
     ]
    }
   ],
   "source": [
    "correct2 = 0\n",
    "total2 = test_classify2.shape[0]\n",
    "# loop through dataframe to obtain number correct\n",
    "for row in test_classify2.iterrows():\n",
    "    # if correct add 1 to correct var\n",
    "    if row[1]['Class'] == row[1]['prediction']:\n",
    "        correct2 += 1\n",
    "# print results\n",
    "print('Total:', total2)\n",
    "print('Correct:', correct2)\n",
    "print('Incorrect:', total2 - correct2)\n",
    "print('Accuracy: {}%'.format((correct2/total2)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "     t/p    ham  spam \n",
      "      ham 948.0  19.0 \n",
      "     spam   9.0 138.0 \n",
      "\n",
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      0.98      0.99       967\n",
      "        spam       0.88      0.94      0.91       147\n",
      "\n",
      "    accuracy                           0.97      1114\n",
      "   macro avg       0.93      0.96      0.95      1114\n",
      "weighted avg       0.98      0.97      0.98      1114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(\"Confusion Matrix: \")\n",
    "print_cm(confusion_matrix(test_classify2['Class'],test_classify2['prediction']),['ham','spam'])\n",
    "print(\"\")\n",
    "print(\"Classification Report: \")\n",
    "print(classification_report(test_classify2['Class'],test_classify2['prediction']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, removing the words that occur only once actually hurt our classifier's score accross the board. It was about half the size of the previous classifier, but we did see an increase in our false negative count and decrease in accuracy which is not ideal. I believe that removing these infrequent words led to worse scores accross the board due to the fact that I only ever process and analyze alphabet characters (a-z). All other words containing non-alpha characters are thrown out. This prevents weird symbols from getting in but then makes the infrequent words important to classification. It wasn't a terrible classifier but when we included all unique vocab it performed better. Overall, there are a many approaches to this, and I have explored two, one of which worked out quite well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
